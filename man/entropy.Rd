% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Distributions.R
\name{entropy}
\alias{entropy}
\alias{H}
\alias{entropy.probability}
\alias{entropy.numeric}
\alias{entropy.density}
\alias{entropy.default}
\alias{xentropy}
\alias{kld}
\alias{kld.probability}
\alias{kld.default}
\alias{mutual}
\alias{mutual.probability}
\alias{mutual.default}
\title{Calculate Entropy or Information Content of variables}
\usage{
entropy(..., model, base = 2)

H(..., model, base = 2)

\method{entropy}{probability}(pdist, model, condition = NULL, base = 2)

\method{entropy}{numeric}(x, model, base = 2, na.rm = TRUE, ...)

\method{entropy}{density}(x, model, base = 2, na.rm = TRUE)

\method{entropy}{default}(..., model, base = 2)

xentropy(..., model, base = 2)

kld(..., model, base = 2)

\method{kld}{probability}(pdist, model, condition = NULL, base = 2)

\method{kld}{default}(..., model, base = 2)

mutual(..., base = 2)

\method{mutual}{probability}(x, base = 2)

\method{mutual}{default}(..., base = 2)

\method{mutual}{probability}(x, base = 2)

\method{mutual}{default}(..., base = 2)
}
\description{
Information content and entropy are fundamental concepts in \href{https://en.wikipedia.org/wiki/Information_theory}{information theory},
which quantify the amount of information (or "surprise") in a random variable.
Both concepts are closely related the probability density/mass of events: improbable events have higher information content.
The probability of \emph{each} observation maps to the \href{https://en.wikipedia.org/wiki/Information_content}{information content};
The average information content of a variable is the \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{entropy}.
Information content/entropy can be calculated for discrete probabilities or continuous probabilities,
and humdrumR defines methods for calculating both.
}
\details{
To calculate information content or entropy, we must assume (or estimate) a probability distribution.
HumdrumR uses R's standard \code{\link[=table]{table()}} and \code{\link[=density]{density()}} functions to estimate discrte and continuous probability
distributions respectively.

Entropy is the average information content of a variable.
The \code{entropy()} function can accept either a \code{\link[=table]{table()}} object (for discrete variables),
or a \code{\link[=density]{density()}} object (for continuous variables).
If \code{entropy()} is passed an \link[base:vector]{atomic} vector,
the values of the vector are treated as observations or a random variable:
for \code{numeric} vectors, the \code{\link[stats:density]{stats::density()}} function is used to estimate the probability distribution
of the random (continuous) variable, then entropy is computed for the density.
For other atomic vectors, \code{\link[=table]{table()}} is called to tabulate the discrete probability mass for each
observed level, and entropy is then computed for the table.

The \code{info()} function only accepts atomic vectors as its main (\code{x}) argument, but must also
be provided a \code{model} argument.
By default, the \code{model} argument is estimated using \code{\link[=density]{density()}} (\code{numeric} input) or \code{\link[=table]{table()}} (other input).
}
\concept{{Information theory functions}}
