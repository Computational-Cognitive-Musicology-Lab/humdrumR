% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Distributions.R
\name{ic}
\alias{ic}
\alias{entropy}
\alias{H}
\alias{entropy,probability-method}
\alias{entropy,count-method}
\alias{entropy,density-method}
\alias{entropy,table-method}
\title{Calculate Entropy or Information Content of variables}
\usage{
ic(x, ...)

entropy(x, base = 2, ...)

entropy(x, base = 2, ...)

\S4method{entropy}{probability}(x, base = 2)

\S4method{entropy}{count}(x, base = 2, ...)

\S4method{entropy}{density}(x, base = 2, na.rm = TRUE)

\S4method{entropy}{table}(x, base = 2, margin = NULL, na.rm = FALSE)
}
\description{
Information content and entropy are fundamental concepts in \href{https://en.wikipedia.org/wiki/Information_theory}{information theory},
which quantify the amount of information (or "surprise") in a random variable.
Both concepts are closely related the probability density/mass of events: improbable events have higher information content.
The probability of \emph{each} observation maps to the \href{https://en.wikipedia.org/wiki/Information_content}{information content};
The average information content of a variable is the \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{entropy}.
Information content/entropy can be calculated for discrete probabilities or continuous probabilities,
and humdrumR defines methods for calculating both.
}
\details{
To calculate information content or entropy, we must assume (or estimate) a probability distribution.
HumdrumR uses R's standard \code{\link[=table]{table()}} and \code{\link[=density]{density()}} functions to estimate discrte and continuous probability
distributions respectively.

Entropy is the average information content of a variable.
The \code{entropy()} function can accept either a \code{\link[=table]{table()}} object (for discrete variables),
or a \code{\link[=density]{density()}} object (for continuous variables).
If \code{entropy()} is passed an \link[base:vector]{atomic} vector,
the values of the vector are treated as observations or a random variable:
for \code{numeric} vectors, the \code{\link[stats:density]{stats::density()}} function is used to estimate the probability distribution
of the random (continuous) variable, then entropy is computed for the density.
For other atomic vectors, \code{\link[=table]{table()}} is called to tabulate the discrete probability mass for each
observed level, and entropy is then computed for the table.

The \code{ic()} function only accepts atomic vectors as its main (\code{x}) argument, but must also
be provided a \code{distribution} argument.
By default, the \code{distribution} argument is estimated using \code{\link[=density]{density()}} (\code{numeric} input) or \code{\link[=table]{table()}} (other input).
}
\seealso{
Other {Information theory functions}: 
\code{\link{crossEntropy}()},
\code{\link{mutualInfo}()}
}
\concept{{Information theory functions}}
