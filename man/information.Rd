% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Distributions.R
\name{information}
\alias{information}
\title{Information theory}
\description{
HumdrumR includes some functionality for the computing statistics related to probability
and \strong{information theory}.
}
\details{
Some important information theory quantities, are statistics that characterize a probability \emph{distribution}.
Thus, they are descriptive statistics, which describe a distribution (usually, the distribution of values in your data)
using a single number.
Such information-theoretic descriptive statistics can be computed using the \code{\link[=entropy]{entropy()}}
(joint/conditional entropy), \code{\link[=xentropy]{xentropy()}} (cross entropy), \code{\link[=kld]{kld()}} (Kullbackâ€“Leibler divergence), and  \code{\link[=mutual]{mutual()}} (mutual information) functions.
Other information theory quantities are calculated (data)point-wise: one value for each observation.
Our vectorized information theory functions include \code{\link[=like]{like()}} (likelihood),
\code{info()} (information content), and \code{pmutual()} (pointwise mutual information).

Note that these functions calculate \emph{empirical} statistics---i.e., they describe your data.
They are not (necessarily) estimates of the "true" quantity.
}
