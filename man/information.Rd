% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Distributions.R
\name{information}
\alias{information}
\title{Information theory}
\description{
Many computational musicology analyses rely on probabilistic modeling and \href{https://en.wikipedia.org/wiki/Information_theory}{information theory}.
HumdrumR includes functions to make these sorts of analyses quick and easy.
These functions are closely connected to our \link{distribution} functions, which can be used to calculate/estimate the probability of data observations.
}
\details{
The most fundamental tools of information theory are statistics that characterize probability \emph{distributions}.
Thus, they are descriptive statistics, which describe a distribution (usually, the distribution of values in your data)
using a single number.
Such information-theoretic descriptive statistics can be computed using the \code{\link[=entropy]{entropy()}}
(joint or conditional entropy), \code{\link[=xentropy]{xentropy()}} (cross entropy), \code{\link[=kld]{kld()}} (Kullbackâ€“Leibler divergence), and  \code{\link[=mutual]{mutual()}} (mutual information) functions.
In contrast, other information theory metrics are calculated "point-wise": one value for each data observation.
Our point-wise information theory functions are \code{\link[=like]{like()}} (likelihood),
\code{\link[=info]{info()}} (information content), \code{pentropy()} (pointwise conditional entropy), and \code{pmutual()} (pointwise mutual information).

Note that all of these functions calculate or utilize \emph{empirical} statistics---i.e., they describe \emph{your data}.
They are not (necessarily) representative of the "true" information content in real music.
They may be used as \emph{estimates} of the "true" entropy of music we study, but this assumes that our sample is
representative and that our probabilistic models make sense (i.e., make valid assumptions).
}
