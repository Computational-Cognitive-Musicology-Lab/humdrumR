<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content='Information content and entropy are fundamental concepts in information theory,
quantifying the amount of information in samples from a random variable; they are often
characterized as measures of how "expected" (low information) or "surprising" (high information) data is.
Both concepts are closely related the probability density/mass of events: improbable events have higher information content.
The probability of each (point-wise) observation maps to the information content;
The average information content of a variable is the entropy.
Information content/entropy can be calculated for discrete probabilities or continuous probabilities,
and humdrumR defines methods for calculating both.'><title>Calculate Entropy or Information Content of variables — entropy • humdrumR</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet"><meta property="og:title" content="Calculate Entropy or Information Content of variables — entropy"><meta property="og:description" content='Information content and entropy are fundamental concepts in information theory,
quantifying the amount of information in samples from a random variable; they are often
characterized as measures of how "expected" (low information) or "surprising" (high information) data is.
Both concepts are closely related the probability density/mass of events: improbable events have higher information content.
The probability of each (point-wise) observation maps to the information content;
The average information content of a variable is the entropy.
Information content/entropy can be calculated for discrete probabilities or continuous probabilities,
and humdrumR defines methods for calculating both.'><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary" data-bs-theme="dark"><div class="container">
    <a href="https://gatech.edu" class="external-link"><img src="gt-logo.svg" alt="Georgia Tech" style="height:50px; padding-right:20px;"></a> <a href="https://ccml.gtcmt.gatech.edu" class="external-link"><img src="CCMLbanner.png" alt="CCMLab" style="height:80px; padding-right:20px;"></a>
    <a class="navbar-brand me-2" href="../index.html">humdrumR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.7.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Basics</h6>
    <a class="dropdown-item" href="../articles/GettingStarted.html">Getting started with humdrumR</a>
    <a class="dropdown-item" href="../articles/DataFields.html">HumdrumR Data Fields</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>R/Background</h6>
    <a class="dropdown-item" href="../articles/HumdrumSyntax.html">The humdrum syntax</a>
    <a class="dropdown-item" href="../articles/RPrimer.html">An R primer for humdrumR users</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Data prep</h6>
    <a class="dropdown-item" href="../articles/ReadWrite.html">Reading and writing humdrum data</a>
    <a class="dropdown-item" href="../articles/ComplexSyntax.html">Complex humdrum syntax</a>
    <a class="dropdown-item" href="../articles/Reshaping.html">Shaping humdrum data</a>
    <a class="dropdown-item" href="../articles/Filtering.html">Filtering humdrum data</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Analysis</h6>
    <a class="dropdown-item" href="../articles/Summary.html">Getting to know your humdrum data</a>
    <a class="dropdown-item" href="../articles/Grouping.html">Grouping humdrum data</a>
    <a class="dropdown-item" href="../articles/Context.html">Contextualizing humdrum data</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Musical tools</h6>
    <a class="dropdown-item" href="../articles/PitchAndTonality.html">Pitch and tonality in humdrumR</a>
    <a class="dropdown-item" href="../articles/RhythmAndMeter.html">Time, rhythm, and meter in humdrumR</a>
    <a class="dropdown-item" href="../articles/KeysAndChord.html">Diatonic and tertian sets in humdrumR</a>
  </div>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://ccml.music.gatech.edu/humdrumR">GUI</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/Computational-Cognitive-Musicology-Lab/humdrumR/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Calculate Entropy or Information Content of variables</h1>
      <small class="dont-index">Source: <a href="https://github.com/Computational-Cognitive-Musicology-Lab/humdrumR/blob/HEAD/R/Distributions.R" class="external-link"><code>R/Distributions.R</code></a></small>
      <div class="d-none name"><code>entropy.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Information content and entropy are fundamental concepts in <a href="https://en.wikipedia.org/wiki/Information_theory" class="external-link">information theory</a>,
quantifying the amount of information in samples from a random variable; they are often
characterized as measures of how "expected" (low information) or "surprising" (high information) data is.
Both concepts are closely related the probability density/mass of events: improbable events have higher information content.
The probability of <em>each</em> (point-wise) observation maps to the <a href="https://en.wikipedia.org/wiki/Information_content" class="external-link">information content</a>;
The average information content of a variable is the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" class="external-link">entropy</a>.
Information content/entropy can be calculated for discrete probabilities or continuous probabilities,
and humdrumR defines methods for calculating both.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">entropy</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">H</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for probability</span></span>
<span><span class="fu">entropy</span><span class="op">(</span><span class="va">pdist</span>, <span class="va">model</span>, condition <span class="op">=</span> <span class="cn">NULL</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for numeric</span></span>
<span><span class="fu">entropy</span><span class="op">(</span><span class="va">x</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for density</span></span>
<span><span class="fu">entropy</span><span class="op">(</span><span class="va">x</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for default</span></span>
<span><span class="fu">entropy</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">xentropy</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">kld</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for probability</span></span>
<span><span class="fu">kld</span><span class="op">(</span><span class="va">pdist</span>, <span class="va">model</span>, condition <span class="op">=</span> <span class="cn">NULL</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for default</span></span>
<span><span class="fu">kld</span><span class="op">(</span><span class="va">...</span>, <span class="va">model</span>, base <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">info</span><span class="op">(</span></span>
<span>  <span class="va">...</span>,</span>
<span>  <span class="va">model</span>,</span>
<span>  base <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  condition <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  na.rm <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  .drop <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  binArgs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>...</dt>
<dd><p><em><strong>Distribution (or atomic vectors) to compute entropy/information of.</strong></em></p>
<p>Must either be a distribution object (created by <code><a href="distributions.html">table()</a></code>, <code><a href="https://rdrr.io/r/stats/density.html" class="external-link">density()</a></code>, <code>count()</code>, or <code><a href="pdist.html">pdist()</a></code>), or one
or more atomic vectors of equal length.</p>
<p>If atomic vectors are provided, and <code>model</code> is missing, the atomic vectors are passed to
<code><a href="pdist.html">pdist()</a></code> in order to calculate the <code>model</code>.</p></dd>


<dt>model</dt>
<dd><p><em><strong>The expected probability model.</strong></em></p>
<p>Must either be omitted (not allowed in calls to <code>xentropy()</code>) or must be a probability distribution created by <code><a href="pdist.html">pdist()</a></code>.</p>
<p>In calls to <code>entropy()</code> or <code>info()</code>, if <code>model</code> is missing, <code>...</code> arguments are used to generate the <code>model</code>.</p></dd>


<dt>base</dt>
<dd><p><em><strong>The logarithmic base.</strong></em></p>
<p>Defaults to <code>2</code>, so information is measured in units "bits."</p>
<p>Must be a single, non-zero positive number.</p>
<p>Use <code>base = exp(1)</code> for natural-log "nats," or <code>base = 10</code> for Hartley/"dits".</p></dd>


<dt>condition</dt>
<dd><p><em><strong>Compute conditional entropy/information, conditoned on this variable.</strong></em></p>
<p>Defaults to <code>NULL</code> (no condition), so the joint entropy is calculated.</p>
<p>Must be a non-empty <code>character</code> string, which matches the name of one or of the named variables
in the distribution.</p>
<p>This argument is simply passed to <code><a href="pdist.html">pdist()</a></code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>To calculate information content or entropy, we must assume (or more often, estimate) a probability distribution.
HumdrumR's <code>count()</code> and <code><a href="pdist.html">pdist()</a></code> methods (or R's standard <code><a href="distributions.html">table()</a></code> function) can be used calculate empirical
distributions of atomic data.
For numeric, data we can also use R's standard <code><a href="https://rdrr.io/r/stats/density.html" class="external-link">stats::density()</a></code> function to estimate the continuous probability density.</p>
<p>The <code>entropy()</code> function takes an object representing a probability distribution---ideally a humdrumR <a href="https://rdrr.io/r/stats/Distributions.html" class="external-link">distribution</a> object,
base-R <a href="distributions.html">table</a>, or a <code><a href="https://rdrr.io/r/stats/density.html" class="external-link">density()</a></code> object (for continuous variables)---and returns the entropy, defaulting to base-2 entropy ("bits").
However, if you are lazy, you can pass <code>entropy()</code> our atomic data vectors directly, and it will automatically pass them to the <code><a href="pdist.html">pdist()</a></code>
function for you; for example, if you want to calculate the joint entropy of variables<code>x</code> and <code>y</code> (which must be the same length),
you can call <code>entropy(pdist(x, y))</code>
or just <code>entropy(x, y)</code>.
Other arguments can be provided to <code><a href="pdist.html">pdist()</a></code> as well; notably, if you want to calculate the <em>conditional</em> entropy,
you can, for example, say <code>entropy(x, y, condition = 'y')</code>.</p>
<p>The <code>info()</code> function is used similarly to the calling <code>entropy()</code> directly on data vectors:
anywhere where you can call <code>entropy(x, y)</code>, you can call <code>info(x, y)</code> instead.
The difference is that <code>info()</code> will return a vector of numbers, the same length as the representing the information content of each input observation.
By definition, entropy of the data distribution is the average of all these point-wise information values: thus, <code>mean(info(x, y)) == entropy(x, y)</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="cross-entropy">Cross entropy<a class="anchor" aria-label="anchor" href="#cross-entropy"></a></h2>
    


<p>In many cases, we simply use entropy/information content to describe a set of data.
In this case, the data we observe and the probability model (distribution) are the same---the probability model is the distribution of the data itself.
However, we can also use a <em>different model</em>---in this case, a different probability distribution---to describe data.
We thus get a measure of how well the model fits the data; this is called the <a href="https://en.wikipedia.org/wiki/Cross-entropy" class="external-link">cross entropy</a>.
The minimum cross entropy occurs when the data matches the model exactly, and that minimum is the normal "self" entropy of the model.
If a data matches the model well, the cross entropy will be a bit higher than the self entropy; if the data matches the model poorly,
the cross entropy can be much higher.
The difference between the cross entropy and the self entropy is always positive (or zero), and is called the
<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" class="external-link">Kullback-Leibler Divergence</a>.</p>
<p>To calculate cross entropy, use the <code>xentropy()</code> command.
(The Kullback-Leibler Divergence can be calculated in the same way using the <code>kld()</code> function.)
The <code>xentropy()</code> command works just like the entropy command, except you need to provide it a <code>model</code> argument, which must be
<em>another</em> probability distribution.
Note that that data and the model have to have the <strong>exact</strong> same variable names, or <code>humdrumR</code> will throw an error!
Name your arguments to avoid this (this is illustrated in the example below, where we name everything <code>X</code>).
To illustrate, lets create three sets of data, two of which are similar, and one which is very different:</p>
<p></p><div class="sourceCode"><pre><code><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>dorian <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">'A'</span>, <span class="st">'B'</span>, <span class="st">'C'</span>, <span class="st">'D'</span>, <span class="st">'E'</span>, <span class="st">'F#'</span>, <span class="st">'G'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>sample1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(dorian, N, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="dv">7</span><span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>sample2 <span class="ot">&lt;-</span> <span class="fu">sample</span>(dorian, N, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="dv">7</span><span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>sample3 <span class="ot">&lt;-</span> <span class="fu">sample</span>(dorian, N, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="do">## first the self entropy</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="fu">entropy</span>(<span class="at">X =</span> sample1)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="fu">entropy</span>(<span class="at">X =</span> sample2)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="fu">entropy</span>(<span class="at">X =</span> sample3)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="do">## now the cross entropy</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="fu">xentropy</span>(<span class="at">X =</span> sample1, <span class="at">model =</span> <span class="fu">pdist</span>(<span class="at">X =</span> sample2))</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="fu">xentropy</span>(<span class="at">X =</span> sample2, <span class="at">model =</span> <span class="fu">pdist</span>(<span class="at">X =</span> sample2))</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="fu">xentropy</span>(<span class="at">X =</span> sample3, <span class="at">model =</span> <span class="fu">pdist</span>(<span class="at">X =</span> sample2))</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span></code></pre><p></p></div>
<p><code>sample1</code> and <code>sample2</code> have very similar distributions, so when we use <code>sample2</code> as a model for <code>sample1</code>,
the cross entropy is only slightly higher than the self entropy of <code>sample1</code>.
However, when we use <code>sample2</code> as the model for <code>sample3</code> (which is distributed very differently)
the entropy is quite a lot higher than the entropy of sample 3.</p>
<p>The <code>info()</code> command can also be passed a <code>model</code> argument.
As always, <code>mean(info(x, model)) == xentropy(x, model)</code>.
There is no standard name for this "cross information content."
However, cross entropy/information-content are closely related to the more general concept of <em>likelihood</em> (see the next section).</p>
    </div>
    <div class="section level2">
    <h2 id="likelihood">Likelihood<a class="anchor" aria-label="anchor" href="#likelihood"></a></h2>
    


<p>The output of <code>info()</code> is identical to the log (base 2 by default) of the modeled <a href="https://en.wikipedia.org/wiki/Likelihood_function" class="external-link">likelihood</a>
of each data point, which can be computed using the <code>like()</code> function.
Literally, <code>info(x, base) == log(like(x), base)</code>.
The <code>like()</code> function works just like <code>info()</code>, computing pointwise probabilities for each data point based on the
probability distribution in <code>model</code>.
However, we can use it to, for example, calculate the total <em>log likelihood</em> of data using <code>sum(log(like(...)))</code>.
This value divided by N is the cross entropy (make sure to use the right log base!): <code>-sum(log(like(...), base = 2)) == xentropy(...)</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>The HumdrumR <a href="information.html">information theory</a> overview.</p>
<p>Other Information theory functions: 
<code><a href="entropy_by.html">entropy_by</a>()</code>,
<code><a href="mutual.html">mutual</a>()</code></p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Nathaniel Condit-Schultz, Claire Arthur.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Georgia Tech, <a href="https://music.gatech.edu" class="external-link">School of Music</a>, <a href="https://ccml.gtcmt.gatech.edu" class="external-link">Computational and Cognitive Musicology Lab</a></p>
</div>

    </footer></div>

  

  

  </body></html>

